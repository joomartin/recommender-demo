{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Id                                       4\n",
      "Name           Szombathely karnevál kártya\n",
      "Description               Savaria karnevál\n",
      "City                           Szombathely\n",
      "State                                  Vas\n",
      "Name: 3, dtype: object, Id                                   6\n",
      "Name                      Sárvár fürdő\n",
      "Description    sárvári termál kristály\n",
      "City                            Sárvár\n",
      "State                              Vas\n",
      "Name: 5, dtype: object]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:107: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:127: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:107: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:196: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:193: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:207: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:214: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################\n",
    "# Bejön egy user adott paraméterekkel                                                         #  \n",
    "# Megállíptja, hogy ezek alapján milyen csoportba tartozik                                    #\n",
    "# Lekérdezi a csoportba tartozó többi usert                                                   #\n",
    "# Lekérdezi az összes interakciót, amit a csoport tagjai produkáltak                          #\n",
    "# Ez alapján összegyűjti az érdekes termékeket                                                #\n",
    "# Az interakciók típusa alapján súlyozza és rendezi őket                                      #\n",
    "# Majd ezen pontok alapján ajánl X terméket                                                   #\n",
    "#                                                                                             #\n",
    "# Termékhez is lehet N hasonló ajánlást kérni                                                 #\n",
    "###############################################################################################\n",
    "\n",
    "# TODO\n",
    "# - Ha még nincsenek interakciók, se userek, akkor is kell valahogy ajánlani, vagy ha egy user nem sorolható semmilyen csoportba\n",
    "# - Pontozásba bele venni azt, hogy a user még nem interaktálódott egy termékkel. Ezek kerüljenek előbbre.\n",
    "# - Felvenni megyét is termék attribútumok közé\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem.snowball import HungarianStemmer\n",
    "\n",
    "class Text_Normalizer:\n",
    "    def remove_punct(self, text):\n",
    "        text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "        return text_nopunct\n",
    "    \n",
    "    def remove_numbers(self, text):\n",
    "        text_nonumbers = \"\".join([char for char in text if char not in ['0','1','2','3','4','5','6','7','8','9']])\n",
    "        return text_nonumbers\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # W+ = A-Za-z0-9 vagy -\n",
    "        tokens = re.split('\\W+', text)\n",
    "        return tokens\n",
    "\n",
    "    def remove_stopwords(self, tokenized_list):\n",
    "        stopwords = nltk.corpus.stopwords.words('hungarian')\n",
    "        text = [word for word in tokenized_list if word not in stopwords]\n",
    "        return text\n",
    "\n",
    "    def stemming(self, tokenized_text):\n",
    "        # ez a hun nltk\n",
    "        patterns = 'i$|t$'\n",
    "        stemmer = HungarianStemmer()\n",
    "        #stemmer = nltk.SnowballStemmer('hungarian')\n",
    "        text = [stemmer.stem(word) for word in tokenized_text]\n",
    "        print(text)\n",
    "        return text\n",
    "\n",
    "    def lemmatizing(self, tokenized_text):\n",
    "        wn = nltk.WordNetLemmatizer()\n",
    "        text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "        return text\n",
    "    \n",
    "    def normalize(self, text):\n",
    "        text_clean = self.remove_punct(text)\n",
    "        text_nonumbers = self.remove_numbers(text_clean)\n",
    "        text_tokenized = self.tokenize(text_nonumbers.lower())\n",
    "        text_nostop = self.remove_stopwords(text_tokenized)\n",
    "        \n",
    "        return self.lemmatizing(text_nostop)\n",
    "\n",
    "def get_data():\n",
    "    users = pd.read_csv('users.csv')\n",
    "    items = pd.read_csv('items.csv')\n",
    "    interactions = pd.read_csv('interactions.csv')\n",
    "    \n",
    "    return users, items, interactions\n",
    "\n",
    "def create_classifier(algo):\n",
    "    from sklearn import tree\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    switcher = {\n",
    "        'decisiontree': tree.DecisionTreeClassifier(),\n",
    "        'svc': SVC(gamma='auto'),\n",
    "        'knn': KNeighborsClassifier(n_neighbors=3)\n",
    "    }\n",
    "    \n",
    "    return switcher.get(algo, None)\n",
    "\n",
    "class User_Recommender:\n",
    "    def get_variables(self, dataset):\n",
    "        variables = list(set(dataset['City']))\n",
    "        variables += list(['F','N'])\n",
    "\n",
    "        return variables\n",
    "\n",
    "    # kategorizált adatokat tartalmazó dataframe -ból ad vissza bináris matrixot\n",
    "    def get_normalized_data(self, dataset, variables, remove_label=True):\n",
    "        new_dataset = dataset\n",
    "        for v in variables: new_dataset[v] = pd.Series([0 for _ in range(len(new_dataset))])\n",
    "        columns = ['City','Sex']\n",
    "\n",
    "        for c in columns:\n",
    "            for i,row in new_dataset.iterrows():\n",
    "                if pd.isnull(row[c]): continue\n",
    "                if row[c] in variables: new_dataset.set_value(i, row[c], 1)\n",
    "\n",
    "        removable = columns\n",
    "        if remove_label == True and 'Label' in dataset.columns:\n",
    "            removable += list(['Label'])\n",
    "\n",
    "        if 'Id' in dataset.columns:\n",
    "            removable += list(['Id'])\n",
    "\n",
    "        return new_dataset.drop(columns=removable)\n",
    "\n",
    "    # egy új, ismeretlen userhez ad vissza hasonló usereket\n",
    "    def get_similar_users(self, user):\n",
    "        classifier = create_classifier('decisiontree')\n",
    "\n",
    "        df_users_original, _, _ = get_data()\n",
    "        y = df_users_original.iloc[:,-1].values\n",
    "\n",
    "        variables = self.get_variables(df_users_original)\n",
    "        df_users = self.get_normalized_data(df_users_original, variables)\n",
    "        classifier.fit(df_users.as_matrix(), y)\n",
    "\n",
    "        df_user = pd.DataFrame([user],columns=['City','Age','Sex'])\n",
    "        df_user = self.get_normalized_data(df_user, variables)\n",
    "\n",
    "        group = classifier.predict(df_user)\n",
    "\n",
    "        similar_users = []\n",
    "        for i,u in df_users_original.iterrows():\n",
    "            if u['Label'] == group[0]: similar_users.append(u)\n",
    "\n",
    "        return similar_users\n",
    "\n",
    "    # Termék ID listát ad vissza user interakciók alapján\n",
    "    def get_item_ids_by_users(self, users):\n",
    "        user_ids = []\n",
    "        for u in users:\n",
    "            user_ids.append(u['Id'])\n",
    "\n",
    "        _, items, interactions = get_data()\n",
    "\n",
    "        interactions_by_users = []\n",
    "        item_ids_by_interactions = []\n",
    "\n",
    "        for i,r in interactions.iterrows():\n",
    "            if r['UserId'] in user_ids: \n",
    "                interactions_by_users.append(r)\n",
    "                item_ids_by_interactions.append(r['ItemId'])\n",
    "\n",
    "        item_ids_by_interactions = list(set(item_ids_by_interactions))\n",
    "        item_ids_by_interactions.sort(key = lambda x:self.get_score(x,interactions_by_users), reverse=True)\n",
    "\n",
    "        return item_ids_by_interactions\n",
    "    \n",
    "    def get_score(self, item_id, interactions):\n",
    "        item_info = {'Buy':0, 'View':0}\n",
    "\n",
    "        for inter in interactions: \n",
    "            if inter['ItemId'] == item_id: item_info[inter['Action']] += 1\n",
    "\n",
    "        sum_interactions = len(interactions)\n",
    "        score = item_info['View'] / sum_interactions\n",
    "        score += (item_info['Buy'] / sum_interactions)*2\n",
    "\n",
    "        return score\n",
    "\n",
    "class Item_Recommender:\n",
    "    def get_normalized_data(self, df):\n",
    "        variables = list(set(df['City']))\n",
    "        variables += list(set(df['State']))\n",
    "\n",
    "        all_keywords = []\n",
    "        for keywords in df['Keywords']:        \n",
    "            for word in keywords: \n",
    "                all_keywords.append(word)\n",
    "\n",
    "        variables += list(set(all_keywords))\n",
    "\n",
    "        new_dataset = df\n",
    "        for v in variables: new_dataset[v] = pd.Series([0 for _ in range(len(new_dataset))])\n",
    "        columns = ['City', 'State', 'Keywords']\n",
    "\n",
    "        for c in columns:\n",
    "            for i,row in new_dataset.iterrows():            \n",
    "                if c == 'Keywords':\n",
    "                    for keyword in row[c]:\n",
    "                        if keyword in variables: new_dataset.set_value(i, keyword, 1)                    \n",
    "                else: \n",
    "                    if pd.isnull(row[c]): continue\n",
    "                    if row[c] in variables: new_dataset.set_value(i, row[c], 1)\n",
    "\n",
    "        removable = columns\n",
    "        removable += list(['Name', 'Description'])\n",
    "        return new_dataset.drop(columns=removable)\n",
    "    \n",
    "    def get_similar_item_ids(self, item_id, N=2):\n",
    "        _, items, _ = get_data()\n",
    "        self.add_keywords(items)\n",
    "        normalized_items = self.get_normalized_data(items);\n",
    "\n",
    "        X = normalized_items.as_matrix()\n",
    "        nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto', metric='euclidean').fit(X)\n",
    "\n",
    "        xtest = None\n",
    "        for i,r in normalized_items.iterrows():\n",
    "            if r['Id'] == item_id: xtest = r\n",
    "\n",
    "        xtest = xtest.as_matrix()\n",
    "        xtest = xtest.reshape(1, -1)\n",
    "\n",
    "        distances, indices = nbrs.kneighbors(xtest)\n",
    "        distances = distances[0][1:]\n",
    "\n",
    "        item = None\n",
    "        for i,r in items.iterrows():\n",
    "            if r['Id'] == item_id: item = r\n",
    "\n",
    "        neighbors_tmp = items.iloc[indices[0][1:]]\n",
    "        neighbors = neighbors_tmp.values.tolist()\n",
    "        for i, val in enumerate(neighbors):\n",
    "            neighbors[i].append(distances[i])\n",
    "\n",
    "        neighbors.sort(key = lambda x:self.get_score(item[1], item[3], item[4], x[1], x[3], x[4], x[-1]), reverse=True)\n",
    "\n",
    "        return list(map(lambda x:x[0],neighbors))[:N]\n",
    "    \n",
    "    def get_score(self, main_name, main_city, main_state, name, city, state, distance):\n",
    "        # TODO éleseben itt lehet majd távolságot / megyét vizsgálni\n",
    "        if main_city == city:\n",
    "            location_factor = 3\n",
    "        else:\n",
    "            location_factor = 1\n",
    "            \n",
    "        # Kerdes, hogy melyik fontosabb. A termek tartalma, vagy a lokacio\n",
    "        # Pl ha megvett egy szombathelyi muzeum, egyeb kulturalis jellegu kartyat\n",
    "        # akkor melyik kapjon nagyobb pontot: egy pesti muzem, kulturalis jellegu kartya, vagy egy sarvar furdo\n",
    "        if main_state == state:\n",
    "            location_factor *= 1.9\n",
    "\n",
    "        if fuzz.ratio(main_name,name) > 50 or fuzz.token_set_ratio(main_name,name) > 50:\n",
    "            name_factor = 2\n",
    "        else:\n",
    "            name_factor = 1\n",
    "\n",
    "        score = location_factor * name_factor \n",
    "        if distance != 0.0: score = score / distance\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def add_keywords(self, df):\n",
    "        # TODO \n",
    "        #   - keresni valamilyen hun-nltk packaget, mert sok töltelék, és ragozott formában lévő szó marad meg\n",
    "        \n",
    "        text_normalizer = Text_Normalizer()\n",
    "        df['Keywords'] = (df['Name']+\" \"+df['Description']).apply(lambda x: text_normalizer.normalize(x))\n",
    "        \n",
    "    def recommend_by_item(self, item_id, N=2):\n",
    "        ids = self.get_similar_item_ids(item_id, N)\n",
    "        _, items, _ = get_data()\n",
    "        \n",
    "        recommended_items = []\n",
    "        for idx in ids:\n",
    "            for i,r in items.iterrows():\n",
    "                if r['Id'] == idx: recommended_items.append(r)\n",
    "                \n",
    "        return recommended_items\n",
    "    \n",
    "    def recommend_by_user(self, user_recommender, user, N=2):\n",
    "        similar_users = user_recommender.get_similar_users(user)\n",
    "        item_ids = user_recommender.get_item_ids_by_users(similar_users)\n",
    "\n",
    "        _, items, _ = get_data()\n",
    "\n",
    "        recommended_items = []\n",
    "        for idx in item_ids:\n",
    "            for i,r in items.iterrows():\n",
    "                if r['Id'] == idx: recommended_items.append(r)\n",
    "\n",
    "        return recommended_items[0:N]\n",
    "    \n",
    "user_recommender = User_Recommender()\n",
    "item_recommender = Item_Recommender()\n",
    "\n",
    "new_user = ['Budapest',23,'F']\n",
    "items_by_user = item_recommender.recommend_by_user(user_recommender,new_user,N=2)\n",
    "#print(items_by_user)\n",
    "\n",
    "\n",
    "items_by_item = item_recommender.recommend_by_item(5,N=2)\n",
    "print(items_by_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
